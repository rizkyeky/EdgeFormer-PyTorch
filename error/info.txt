Baca paper ini

Recently, various vision transformers (ViTs) models have achieved remarkable results in many vision tasks, forming strong alternatives to convolutional neural networks (ConvNets) Dosovitskiy et al. 
(2020) Touvron et al. (2021) Liu et al. (2021). However, we believe both ViTs and ConvNets are indispensable for the following reasons: 1) From application perspective, both ViTs and ConvNets have their advantages and disadvantages.
ViT models generally have better performance but usually suffer from high computational cost and are difficult to train Touvron et al. (2021). Compared with ViTs, ConvNets may show inferior performance, but they still have some unique advantages. 
For instance, ConvNets have better hardware support and are easy to train. In addition, as is summarized in Guo et al. (2021) and our experiments, ConvNets still dominate in the area of small models for mobile or edge devices. 2) From the information processing perspective, both ViTs and ConvNets have unique features. 
ViTs are good at extracting global information and use attention mechanism to extract information from different locations driven by input data Chen et al. (2021) Mehta & Rastegari (2022). 
ConvNets focus on modeling local relationships and have strong prior by inductive bias Dai et al. (2021). The above analysis naturally raise a question: can we learn from ViTs to improve ConvNets for mobile or edge computing applications?.

In this paper, we aim to design new light-weight pure ConvNets that further enhance its strength in the area of mobile and edge computing friendly models. Pure convolution is more mobile friendly because convolutions are highly optimized by existing tool chains that are widely used to deploy model into these resource constrained devices. 
Even more, because of the huge popularity of ConvNets in the past few years, some existing neural network accelerators are designed mainly around convolution style operations, and the complex non-linear operations such as softmax and data bus bandwidth demanding large matrix multiplications are not efficiently supported. 
These hardware and software constraints make a pure convolutional light-weight model more preferable even if a ViT based model is equally competitive in other aspects.

To design such a ConvNet, we compare ConvNets with ViTs and summarize three main differences between them: 1) ViTs are good at extracting global features Chen et al. (2021) Mehta & Rastegari (2022) Dai et al. (2021); 2) ViTs adopt Meta-former block Yu et al. (2021); 3) Information aggregations in ViTs are data driven. 
Corresponding to these three points, we design our EdgeFormer block. 1) We propose the global circular convolution (GCC) to extract global features; 2) Based on the proposed GCC, we build a pure ConvNet Meta-former block as the basic outer structure; 3) We add channel wise attention module to the feature forward network (FFN) part of meta-former, which makes our proposed EdgeFormer adapt kernel weights according inputs. 
Finally, inspired by CoatNet Dai et al. (2021) and MobileViT Mehta & Rastegari (2022), we use a bifurcate structure (section 3.2) as the outer frame to build a complete network EdgeFormer. 
Experiment results show that the proposed EdgeFormer achieves solid performance on three popular vision tasks, including image classification, object detection and semantic segmentation. 
Taking experiment results of image classification as an example, EdgeFormer achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11% parameters and 13% computational cost but gaining 0.2% higher accuracy and 23% inference speed (on Rockchip RK3288) compared with MobileViT Mehta & Rastegari (2022). 
For experiments of object detection and semantic segmentation, compared with other light-weight models, the proposed EdgeFormer achieves higher mAP and mIOU, while having fewer parameters.

2017, light-weight ConvNets attract much attentions as more and more applications needs to run ConvNet models on mobile devices. Now, there are a lot of light-weight ConvNets, such as ShuffleNets Ma et al. (2018) Ma et al. (2018), MobileNets Howard et al. (2017) Sandler et al. (2018) Howard et al. (2019), MicroNet Li et al. (2021), GhostNet Han et al. (2020), EfficientNet Tan & Le (2019), TinyNet Chen et al. (2019) and MnasNet Tan et al. (2019). 
Compared with standard ConvNets, light-weight ConvNets have fewer parameters, lower computational cost and faster inference speed. In addition, light-weight ConvNets can be applied on a wide range of devices. Despite these benefits, these light-weight models have inferior performance compared with heavy-weight models. Very recently, following the research line of combining strengths of ConvNet and ViT, some researcher attempted to build light-weight hybrid models for mobile vision tasks. 
Mobile-Former presents a parallel design of MobileNet and transformer, which leverages the advantages of MobileNet at extracting local features and transformer at capturing global information Chen et al. (2021). Mehta and Rastegari proposed MobileViT, where the upper stages of MobileNetv2 Sandler et al. (2018) are replaced with MobileViT block Mehta & Rastegari (2022). 
In MobileViT block, local representations extracted by convolution and global representations are concatenated to generate local-global representations. In terms of purpose, our proposed EdgeFormer is related to Mobile-Former and MobileViT. Different from these two models which still keep transformer blocks, our proposed EdgeFormer is pure ConvNet, which makes our proposed EdgeFormer more mobile friendly. Our experiments of deploying models on low power platform confirm this point. 
In terms of designing a pure ConvNet via learning from ViTs, our work is most closely related to a parallel work ConvNext Liu et al. (2022). The two major differences are: 1) Ideas and architectures are different. The ConvNext modernizes a standard ResNet toward the design of a vision transformer by introducing a series (more than ten) of incremented but effective designs. 
Our proposed EdgeFormer starts from three main differences between ConvNets and ViTs and fills the gaps from macro level. As the ideas are different, the corresponding structures are also different; 2) They are proposed for different purposes. 
Our EdgeFormer is proposed for mobile devices. Compared with ConvNext, the proposed EdgeFormer shows advantages when constraining models as light-weight models. Corresponding experiment results are listed in 4.5

dari paragraf tersebut, simpulkan apa itu edgeformer